{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: convert to .py script \n",
    "\n",
    "import pickle as pkl\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from bert import extract_features_manifolds as extract_features\n",
    "from bert import tokenization\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"models\\\\bert-prune-30-squad\"\n",
    "experiment_dir = \"experiments\\\\bert_experiment_test\"\n",
    "input_pkl_filename = \"test_questions_final_2.pkl\"\n",
    "relevant_pos_filename = \"relevant_pos_tags.txt\"\n",
    "sample_filename = \"sample_seed_0.pkl\"\n",
    "\n",
    "num_hidden_layers = 12\n",
    "\n",
    "# Model directory must contain the following files:\n",
    "#     - bert_config.json\n",
    "#     - vocab.txt\n",
    "#     - model.ckpt.data-00000-of-00001\n",
    "#     - model.ckpt.index\n",
    "#     - model.ckpt.meta\n",
    "#     - checkpoint\n",
    "# \n",
    "# \"checkpoint\" file contains the following line:\n",
    "#     model_checkpoint_path: \"model.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Restructure Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment directories\n",
    "inputs_dir = os.path.join(experiment_dir, \"input\")\n",
    "intermediate_dir = os.path.join(experiment_dir, \"intermediate\")\n",
    "features_dir = os.path.join(experiment_dir, \"features\")\n",
    "mftma_dir = os.path.join(experiment_dir, \"mftma-analysis\")\n",
    "\n",
    "# Make the directories if they don't exist\n",
    "os.makedirs(inputs_dir, exist_ok=True)\n",
    "os.makedirs(intermediate_dir, exist_ok=True)\n",
    "os.makedirs(features_dir, exist_ok=True)\n",
    "os.makedirs(mftma_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "input_pkl_file = os.path.join(inputs_dir, input_pkl_filename)\n",
    "sample_pkl_file = os.path.join(inputs_dir, sample_filename)\n",
    "\n",
    "tagged_data_obj = pkl.load(open(input_pkl_file, 'rb'))\n",
    "relevant_pos = open(os.path.join(inputs_dir, relevant_pos_filename), 'r').read().splitlines()\n",
    "line_word_tag_map = pkl.load(open(sample_pkl_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_examples = []\n",
    "INPUT_sentences = []\n",
    "INPUT_key_tag = []\n",
    "\n",
    "for ex in tagged_data_obj:\n",
    "        tags = []\n",
    "        words = []\n",
    "        for word_tag in ex[2]:\n",
    "            # Some words are not tagged (looks like only empty strings)\n",
    "            if len(word_tag)==2:\n",
    "                words.append(word_tag[0].lower())\n",
    "                tags.append(word_tag[1].lower())\n",
    "        sentence = \" \".join(words)\n",
    "\n",
    "        INPUT_examples.append({\"words\": words, \"tags\": tags})\n",
    "        INPUT_sentences.append(sentence)\n",
    "        INPUT_key_tag.append(f\"{ex[0]}^{ex[1]}\")\n",
    "    \n",
    "    \n",
    "print(f\"Found {len(INPUT_examples)} examples\")\n",
    "\n",
    "# (Limit during development to save time)\n",
    "#limit_examples = 400 \n",
    "#INPUT_examples = INPUT_examples[:limit_examples]\n",
    "#INPUT_sentences = INPUT_sentences[:limit_examples]\n",
    "#INPUT_key_tag = INPUT_key_tag[:limit_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_file = os.path.join(intermediate_dir, \"bert_input_sentences.txt\")\n",
    "\n",
    "# Write the input sentences to a file\n",
    "with open(sentences_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for sentence in INPUT_sentences:\n",
    "        f.write(f\"{sentence}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model files\n",
    "bert_config_file = os.path.join(model_dir, \"bert_config.json\")\n",
    "vocab_file = os.path.join(model_dir, \"vocab.txt\")\n",
    "init_checkpoint = model_dir\n",
    "\n",
    "# Layer indices\n",
    "num_layers = 12\n",
    "layers = range(num_layers)\n",
    "layers_str = \",\".join([str(l) for l in layers])\n",
    "\n",
    "# Target file\n",
    "features_file = os.path.join(intermediate_dir, \"bert_features.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts features of the sentences in file sentences_file, records them\n",
    "#   in file features_file\n",
    "extract_features.extract(input_file=sentences_file,\n",
    "                         output_file=features_file,\n",
    "                         bert_config_file=bert_config_file,\n",
    "                         init_checkpoint=init_checkpoint,\n",
    "                         vocab_file=vocab_file,\n",
    "                         layers=layers_str\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restructure Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previously saved features\n",
    "features_jsons = []\n",
    "with open(features_file, 'r') as fp:\n",
    "    features_jsons.extend(json.loads(line) for line in fp)\n",
    "    \n",
    "# Getting feature vectors out of features_jsons:\n",
    "# features_jsons[i]['features'][j]['layers'][k]['values']\n",
    "# i = example\n",
    "# j = token\n",
    "# k = layer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=True)\n",
    "\n",
    "# initialise the dict structure\n",
    "manifold_vectors = defaultdict(dict)\n",
    "for tag in relevant_pos:\n",
    "    tag = tag.strip().lower()\n",
    "    for layer in range(1, num_hidden_layers + 1):\n",
    "        manifold_vectors[layer][tag] = None\n",
    "\n",
    "        \n",
    "# Iterate over the examples + their data structure\n",
    "# Fill in the corresponding elements in manifold_vectors\n",
    "\n",
    "# Relying on the feature extraction above running on ALL lines -> indexing preserved\n",
    "for line_idx, (features_dict, key_tag) in enumerate(zip(features_jsons, INPUT_examples)): \n",
    "    if line_idx in line_word_tag_map:\n",
    "        word_list = key_tag['words']\n",
    "        for word_idx in line_word_tag_map[line_idx]: \n",
    "            tag = line_word_tag_map[line_idx][word_idx]\n",
    "            # appending -1 to the 1st and last position to represent CLS and SEP tokens \n",
    "            split_word_idx = [-1]\n",
    "            # tokenization - assign the same id for all sub words of a same word\n",
    "            for split_id, split_word in enumerate(word_list):\n",
    "                tokens = tokenizer.tokenize(split_word)\n",
    "                split_word_idx.extend([split_id] * len(tokens))\n",
    "            split_word_idx.append(-1)\n",
    "            \n",
    "            vector_idcs = np.argwhere(np.array(split_word_idx) == word_idx).reshape(-1)         \n",
    "            tokens_features = [features_dict['features'][i]['layers'] for i in vector_idcs]\n",
    "            \n",
    "            # iterating through layers in mftma encoding (1-12)\n",
    "            for layer in range(1, num_hidden_layers + 1):     \n",
    "                tokens_layer_features = [token_features[layer-1]['values'] for token_features in tokens_features]\n",
    "                # take the mean of token features of the same word to represent the word as a single feature vector\n",
    "                token_vector = np.mean(tokens_layer_features, axis=0).reshape(-1,1)\n",
    "                if manifold_vectors[layer][tag] is None:\n",
    "                    manifold_vectors[layer][tag] = token_vector\n",
    "                else:\n",
    "                    manifold_vectors[layer][tag] = np.hstack((manifold_vectors[layer][tag], token_vector))                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Feature Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in range(1,num_hidden_layers+1):\n",
    "    pkl.dump(list(manifold_vectors[layer].values()), open(os.path.join(features_dir,\n",
    "                                                                  str(layer)+'.pkl'), 'wb+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command to generate mftma files\n",
    "# python mftma_analysis.py --feature_dir experiments/bert_experiment_test/features --mftma_analysis_dir experiments/bert_experiment_test/mftma-analysis  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
