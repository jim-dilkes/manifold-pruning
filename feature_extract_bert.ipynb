{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: convert to .py script \n",
    "\n",
    "import pickle as pkl\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from bert import extract_features_manifolds as extract_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"models\\\\bert-prune-30-squad\"\n",
    "experiment_dir = \"experiments\\\\bert_experiment_test\"\n",
    "input_pkl_filename = \"train_questions_final.pkl\"\n",
    "relevant_pos_filename = \"relevant_pos.txt\"\n",
    "\n",
    "max_manifold_size = 50 # Not yet implemented\n",
    "\n",
    "# Model directory must contain the following files:\n",
    "#     - bert_config.json\n",
    "#     - vocab.txt\n",
    "#     - model.ckpt.data-00000-of-00001\n",
    "#     - model.ckpt.index\n",
    "#     - model.ckpt.meta\n",
    "#     - checkpoint\n",
    "# \n",
    "# \"checkpoint\" file contains the following line:\n",
    "#     model_checkpoint_path: \"model.ckpt\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Restructure Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment directories\n",
    "inputs_dir = os.path.join(experiment_dir, \"input\")\n",
    "intermediate_dir = os.path.join(experiment_dir, \"intermediate\")\n",
    "features_dir = os.path.join(experiment_dir, \"features\")\n",
    "mftma_dir = os.path.join(experiment_dir, \"mftma-analysis\")\n",
    "\n",
    "# Make the directories if they don't exist\n",
    "os.makedirs(inputs_dir, exist_ok=True)\n",
    "os.makedirs(intermediate_dir, exist_ok=True)\n",
    "os.makedirs(features_dir, exist_ok=True)\n",
    "os.makedirs(mftma_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "input_pkl_file = os.path.join(inputs_dir, input_pkl_filename)\n",
    "\n",
    "tagged_data_obj = pkl.load(open(input_pkl_file, 'rb'))\n",
    "relevant_pos = open(os.path.join(inputs_dir, relevant_pos_filename), 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1079 examples\n"
     ]
    }
   ],
   "source": [
    "INPUT_examples = []\n",
    "INPUT_sentences = []\n",
    "INPUT_key_tag = []\n",
    "\n",
    "for ex in tagged_data_obj:\n",
    "    tags = []\n",
    "    words = []\n",
    "    for word_tag in ex[2]:\n",
    "        # Some words are not tagged (looks like only empty strings)\n",
    "        if len(word_tag)==2:\n",
    "            words.append(word_tag[0])\n",
    "            tags.append(word_tag[1])\n",
    "    sentence = \" \".join(words)\n",
    "    \n",
    "    INPUT_examples.append({\"words\": words, \"tags\": tags})\n",
    "    INPUT_sentences.append(sentence)\n",
    "    INPUT_key_tag.append(f\"{ex[0]}^{ex[1]}\")\n",
    "    \n",
    "print(f\"Found {len(INPUT_examples)} examples\")\n",
    "\n",
    "\n",
    "# (Limit during development to save time)\n",
    "limit_examples = 400 \n",
    "INPUT_examples = INPUT_examples[:limit_examples]\n",
    "INPUT_sentences = INPUT_sentences[:limit_examples]\n",
    "INPUT_key_tag = INPUT_key_tag[:limit_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_file = os.path.join(intermediate_dir, \"bert_input_sentences.txt\")\n",
    "\n",
    "# Write the input sentences to a file\n",
    "with open(sentences_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for sentence in INPUT_sentences:\n",
    "        f.write(f\"{sentence}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model files\n",
    "bert_config_file = os.path.join(model_dir, \"bert_config.json\")\n",
    "vocab_file = os.path.join(model_dir, \"vocab.txt\")\n",
    "init_checkpoint = model_dir\n",
    "\n",
    "# Layer indices\n",
    "num_layers = 12\n",
    "layers = range(num_layers)\n",
    "layers_str = \",\".join([str(l) for l in layers])\n",
    "\n",
    "# Target file\n",
    "features_file = os.path.join(intermediate_dir, \"bert_features.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts features of the sentences in file sentences_file, records them\n",
    "#   in file features_file\n",
    "extract_features.extract(input_file=sentences_file,\n",
    "                         output_file=features_file,\n",
    "                         bert_config_file=bert_config_file,\n",
    "                         init_checkpoint=init_checkpoint,\n",
    "                         vocab_file=vocab_file,\n",
    "                         layers=layers_str\n",
    "                         )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restructure Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previously saved features\n",
    "features_jsons = []\n",
    "with open(features_file, 'r') as fp:\n",
    "    features_jsons.extend(json.loads(line) for line in fp)\n",
    "    \n",
    "# Getting feature vectors out of features_jsons:\n",
    "# features_jsons[i]['features'][j]['layers'][k]['values']\n",
    "# i = example\n",
    "# j = token\n",
    "# k = layer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Limit the number of feature vectors per pos tag\n",
    "\n",
    "# tag -> relevant_pos index mapping\n",
    "tag_idx_map = {tag: i for i, tag in enumerate(relevant_pos)}\n",
    "\n",
    "\n",
    "# Generate data structure\n",
    "# -> list for each layer\n",
    "# -> list for each pos tags per layer\n",
    "# -> list for each example per pos tag per layer\n",
    "# features = [layer][pos_tag][example]\n",
    "features_structure = [[[] for _ in range(len(relevant_pos))] for _ in range(num_layers)]\n",
    "\n",
    "\n",
    "# Iterate over the examples + their data structure\n",
    "# Fill in the corresponding elements in features_structure\n",
    "for features_dict, key_tag in zip(features_jsons, INPUT_examples):\n",
    "    for layers_dict, tag in zip(features_dict['features'], key_tag['tags']):\n",
    "        if tag in relevant_pos:\n",
    "            for layer_number, layer_features in enumerate(layers_dict['layers']):\n",
    "                features_structure[layer_number][tag_idx_map[tag]].append(layer_features['values'])\n",
    "                "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Feature Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the inner lists to numpy arrays of the same length\n",
    "# Sample to the min(max_manifold_size, len(array)) elements\n",
    "\n",
    "# Count the number of examples per pos tag for the first layer\n",
    "# This is the same for all layers\n",
    "arr_size = min(min(\n",
    "    len(tag_examples) for tag_examples in features_structure[0]\n",
    "), max_manifold_size)\n",
    "\n",
    "\n",
    "# Arrays \n",
    "for layer_number, tag_features in enumerate(features_structure,1):\n",
    "    layer_tag_arrays = [(np.array(tag[:arr_size])).T for tag in tag_features]\n",
    "    # Record to file\n",
    "    pkl.dump(layer_tag_arrays, open(os.path.join(features_dir, f\"{layer_number}.pkl\"), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command to generate mftma files\n",
    "# python mftma_analysis.py --feature_dir experiments/bert_experiment_test/features --mftma_analysis_dir experiments/bert_experiment_test/mftma-analysis  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
