{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"bert-base-uncased\"\n",
    "ckpt_dir = \"./bert-base-uncased/\"\n",
    "mask_dir = \"./masks/bert-base-uncased/squad/mac/0.5/seed_0/\"\n",
    "# data_file = \"./dataset/cardinal.pkl\"\n",
    "data_file = \"./dataset/ptb_pos.txt\"\n",
    "sample_file = \"./dataset/sample_seed_0.pkl\"\n",
    "tag_file = \"./dataset/relevant_pos.txt\"\n",
    "output_dir = \"./features\"\n",
    "RUN_MASKED = False\n",
    "IS_SQUAD = True\n",
    "\n",
    "# for reproducibility\n",
    "set_seed(0) # handles torch, np, random, tf in theory as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert-base-uncased/ were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ./bert-base-uncased/ and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load the finetuned model and the corresponding tokenizer\n",
    "config = AutoConfig.from_pretrained(ckpt_dir, output_hidden_states=True)\n",
    "model_generator = AutoModelForQuestionAnswering if IS_SQUAD else AutoModelForSequenceClassification\n",
    "model = model_generator.from_pretrained(ckpt_dir, config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    use_fast=True,\n",
    "    use_auth_token=None,\n",
    ")\n",
    "\n",
    "# Load masks\n",
    "head_mask = torch.load(mask_dir + \"head_mask.pt\")\n",
    "neuron_mask = torch.load(mask_dir + \"neuron_mask.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers for applying neuron mask\n",
    "def get_layers(model):\n",
    "    model_type = model.base_model_prefix\n",
    "    backbone = getattr(model, model_type)\n",
    "    encoder = backbone.encoder\n",
    "    layers = encoder.layer\n",
    "    return layers\n",
    "\n",
    "def get_ffn2(model, index):\n",
    "    layer = get_layers(model)[index]\n",
    "    ffn2 = layer.output\n",
    "    return ffn2\n",
    "\n",
    "def register_mask(module, mask):\n",
    "    hook = lambda _, inputs: (inputs[0] * mask, inputs[1])\n",
    "    handle = module.register_forward_pre_hook(hook)\n",
    "    return handle\n",
    "\n",
    "def apply_neuron_mask(model, neuron_mask):\n",
    "    num_hidden_layers = neuron_mask.shape[0]\n",
    "    handles = []\n",
    "    for layer_idx in range(num_hidden_layers):\n",
    "        ffn2 = get_ffn2(model, layer_idx)\n",
    "        handle = register_mask(ffn2, neuron_mask[layer_idx])\n",
    "        handles.append(handle)\n",
    "    return handles\n",
    "\n",
    "def remove_neuron_mask(handles):\n",
    "    for handle in handles:\n",
    "        handle.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model and apply neuron mask\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "handles = apply_neuron_mask(model, neuron_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove neuron mask\n",
    "# remove_neuron_mask(handles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifold_vectors = defaultdict(dict)\n",
    "with open(tag_file) as f:\n",
    "    for tag in f:\n",
    "        tag = tag.strip().lower()\n",
    "        for layer in range(1,config.num_hidden_layers+1):\n",
    "            manifold_vectors[layer][tag] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_word_tag_map = pkl.load(open(sample_file, 'rb+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_file, encoding='utf-8') as dfile:\n",
    "    for line_idx,line in enumerate(dfile):\n",
    "        if line_idx in line_word_tag_map:\n",
    "            words, tags = line.strip().split('\\t')\n",
    "            word_list = list(words.split())\n",
    "            for word_idx in line_word_tag_map[line_idx]:\n",
    "                tag = line_word_tag_map[line_idx][word_idx].lower()\n",
    "                if RUN_MASKED:\n",
    "                    # replace the word_idx location with mask token\n",
    "                    word_list[word_idx] = tokenizer.mask_token\n",
    "\n",
    "                if model_name == 'openai-gpt':\n",
    "                    split_word_idx = []\n",
    "                else:\n",
    "                    split_word_idx = [-1]\n",
    "\n",
    "                # tokenization - assign the same id for all sub words of a same word\n",
    "                word_tokens = []\n",
    "                for split_id, split_word in enumerate(word_list):\n",
    "                    tokens = tokenizer.tokenize(split_word)\n",
    "                    word_tokens.extend(tokens)\n",
    "                    split_word_idx.extend([split_id] * len(tokens))\n",
    "\n",
    "                if model_name != 'openai-gpt':\n",
    "                    split_word_idx.append(len(word_list))\n",
    "                # print(word_tokens)\n",
    "\n",
    "                input_ids = torch.Tensor([tokenizer.encode(text=word_tokens, is_split_into_words=True, add_special_tokens=True)]).long()\n",
    "                input_ids = input_ids.to(device)\n",
    "                with torch.no_grad():\n",
    "                    model_output = model(input_ids, head_mask=head_mask)[-1]\n",
    "                for layer in range(1,config.num_hidden_layers+1):\n",
    "                    layer_output = model_output[layer][0]\n",
    "                    vector_idcs = np.argwhere(np.array(split_word_idx) == word_idx).reshape(-1)\n",
    "                    token_vector = layer_output[vector_idcs].mean(0).cpu().reshape(-1,1).numpy()\n",
    "                    if manifold_vectors[layer][tag] is None:\n",
    "                        manifold_vectors[layer][tag] = token_vector\n",
    "                    else:\n",
    "                        manifold_vectors[layer][tag] = np.hstack((manifold_vectors[layer][tag], token_vector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save embedding vectors\n",
    "for layer in range(1,config.num_hidden_layers+1):\n",
    "    pkl.dump(list(manifold_vectors[layer].values()), open(os.path.join(output_dir, str(layer)+'.pkl'), 'wb+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
